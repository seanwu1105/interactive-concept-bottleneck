{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.concept_bottleneck.dataset import (\n",
    "    CUB200AttributesToClass,\n",
    "    NUM_ATTRIBUTES,\n",
    "    NUM_CLASSES,\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "training_data = CUB200AttributesToClass(train=True)\n",
    "test_data = CUB200AttributesToClass(train=False)\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.ops import MLP\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MLP(\n",
    "    in_channels=NUM_ATTRIBUTES,\n",
    "    hidden_channels=[NUM_CLASSES],\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader[tuple[npt.NDArray[np.float32], np.int_]],\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "):\n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader[tuple[npt.NDArray[np.float32], np.int_]],\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: str,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            test_loss += loss_fn(logits, y).item()\n",
    "            correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)  # type: ignore\n",
    "\n",
    "    return test_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100-------------------\n",
      "Training Loss: 2.7229, Training Accuracy: 47.0637%\n",
      "Test Loss: 3.2976, Test Accuracy: 30.8595%\n",
      "Saving model to attributes-to-class.pth with accuracy 30.8595%\n",
      "Epoch 2/100-------------------\n",
      "Training Loss: 2.0053, Training Accuracy: 62.8795%\n",
      "Test Loss: 2.8224, Test Accuracy: 39.1094%\n",
      "Saving model to attributes-to-class.pth with accuracy 39.1094%\n",
      "Epoch 3/100-------------------\n",
      "Training Loss: 1.6295, Training Accuracy: 69.2359%\n",
      "Test Loss: 2.6060, Test Accuracy: 41.7328%\n",
      "Saving model to attributes-to-class.pth with accuracy 41.7328%\n",
      "Epoch 4/100-------------------\n",
      "Training Loss: 1.3737, Training Accuracy: 73.8572%\n",
      "Test Loss: 2.4832, Test Accuracy: 43.3207%\n",
      "Saving model to attributes-to-class.pth with accuracy 43.3207%\n",
      "Epoch 5/100-------------------\n",
      "Training Loss: 1.1960, Training Accuracy: 76.8769%\n",
      "Test Loss: 2.4110, Test Accuracy: 44.4598%\n",
      "Saving model to attributes-to-class.pth with accuracy 44.4598%\n",
      "Epoch 6/100-------------------\n",
      "Training Loss: 1.0583, Training Accuracy: 79.7798%\n",
      "Test Loss: 2.3769, Test Accuracy: 44.5116%\n",
      "Saving model to attributes-to-class.pth with accuracy 44.5116%\n",
      "Epoch 7/100-------------------\n",
      "Training Loss: 0.9466, Training Accuracy: 81.9319%\n",
      "Test Loss: 2.3591, Test Accuracy: 45.4263%\n",
      "Saving model to attributes-to-class.pth with accuracy 45.4263%\n",
      "Epoch 8/100-------------------\n",
      "Training Loss: 0.8663, Training Accuracy: 83.8172%\n",
      "Test Loss: 2.3666, Test Accuracy: 45.8060%\n",
      "Saving model to attributes-to-class.pth with accuracy 45.8060%\n",
      "Epoch 9/100-------------------\n",
      "Training Loss: 0.7813, Training Accuracy: 85.7691%\n",
      "Test Loss: 2.3635, Test Accuracy: 45.9441%\n",
      "Saving model to attributes-to-class.pth with accuracy 45.9441%\n",
      "Epoch 10/100-------------------\n",
      "Training Loss: 0.7262, Training Accuracy: 86.7367%\n",
      "Test Loss: 2.3875, Test Accuracy: 45.8060%\n",
      "Epoch 11/100-------------------\n",
      "Training Loss: 0.6715, Training Accuracy: 87.6043%\n",
      "Test Loss: 2.4067, Test Accuracy: 45.4953%\n",
      "Epoch 12/100-------------------\n",
      "Training Loss: 0.6200, Training Accuracy: 89.4728%\n",
      "Test Loss: 2.4337, Test Accuracy: 46.1857%\n",
      "Saving model to attributes-to-class.pth with accuracy 46.1857%\n",
      "Epoch 13/100-------------------\n",
      "Training Loss: 0.5842, Training Accuracy: 89.4394%\n",
      "Test Loss: 2.4548, Test Accuracy: 45.9959%\n",
      "Epoch 14/100-------------------\n",
      "Training Loss: 0.5466, Training Accuracy: 90.3237%\n",
      "Test Loss: 2.4843, Test Accuracy: 45.8233%\n",
      "Epoch 15/100-------------------\n",
      "Training Loss: 0.5076, Training Accuracy: 91.4915%\n",
      "Test Loss: 2.4949, Test Accuracy: 46.3928%\n",
      "Saving model to attributes-to-class.pth with accuracy 46.3928%\n",
      "Epoch 16/100-------------------\n",
      "Training Loss: 0.4739, Training Accuracy: 92.6593%\n",
      "Test Loss: 2.5390, Test Accuracy: 45.9096%\n",
      "Epoch 17/100-------------------\n",
      "Training Loss: 0.4512, Training Accuracy: 92.9596%\n",
      "Test Loss: 2.5616, Test Accuracy: 46.0131%\n",
      "Epoch 18/100-------------------\n",
      "Training Loss: 0.4274, Training Accuracy: 93.2599%\n",
      "Test Loss: 2.6016, Test Accuracy: 45.4436%\n",
      "Epoch 19/100-------------------\n",
      "Training Loss: 0.4033, Training Accuracy: 93.9273%\n",
      "Test Loss: 2.6283, Test Accuracy: 45.0121%\n",
      "Epoch 20/100-------------------\n",
      "Training Loss: 0.3854, Training Accuracy: 93.9439%\n",
      "Test Loss: 2.6665, Test Accuracy: 45.5299%\n",
      "Epoch 21/100-------------------\n",
      "Training Loss: 0.3654, Training Accuracy: 94.5279%\n",
      "Test Loss: 2.6911, Test Accuracy: 45.4436%\n",
      "Epoch 22/100-------------------\n",
      "Training Loss: 0.3441, Training Accuracy: 94.8615%\n",
      "Test Loss: 2.7196, Test Accuracy: 45.2019%\n",
      "Epoch 23/100-------------------\n",
      "Training Loss: 0.3353, Training Accuracy: 94.8949%\n",
      "Test Loss: 2.7815, Test Accuracy: 44.9776%\n",
      "Epoch 24/100-------------------\n",
      "Training Loss: 0.3135, Training Accuracy: 95.4955%\n",
      "Test Loss: 2.7846, Test Accuracy: 45.0639%\n",
      "Epoch 25/100-------------------\n",
      "Training Loss: 0.2992, Training Accuracy: 95.8959%\n",
      "Test Loss: 2.8180, Test Accuracy: 44.9258%\n",
      "Epoch 26/100-------------------\n",
      "Training Loss: 0.2842, Training Accuracy: 96.2296%\n",
      "Test Loss: 2.8601, Test Accuracy: 44.8050%\n",
      "Epoch 27/100-------------------\n",
      "Training Loss: 0.2744, Training Accuracy: 96.0794%\n",
      "Test Loss: 2.8923, Test Accuracy: 44.3217%\n",
      "Epoch 28/100-------------------\n",
      "Training Loss: 0.2701, Training Accuracy: 96.0294%\n",
      "Test Loss: 2.9313, Test Accuracy: 44.8740%\n",
      "Epoch 29/100-------------------\n",
      "Training Loss: 0.2565, Training Accuracy: 96.5966%\n",
      "Test Loss: 2.9609, Test Accuracy: 44.5461%\n",
      "Epoch 30/100-------------------\n",
      "Training Loss: 0.2460, Training Accuracy: 96.6466%\n",
      "Test Loss: 3.0032, Test Accuracy: 44.2009%\n",
      "Epoch 31/100-------------------\n",
      "Training Loss: 0.2326, Training Accuracy: 96.9469%\n",
      "Test Loss: 3.0280, Test Accuracy: 44.4770%\n",
      "Epoch 32/100-------------------\n",
      "Training Loss: 0.2328, Training Accuracy: 96.7134%\n",
      "Test Loss: 3.0770, Test Accuracy: 44.2699%\n",
      "Epoch 33/100-------------------\n",
      "Training Loss: 0.2167, Training Accuracy: 97.2139%\n",
      "Test Loss: 3.1031, Test Accuracy: 44.3735%\n",
      "Epoch 34/100-------------------\n",
      "Training Loss: 0.2175, Training Accuracy: 97.1138%\n",
      "Test Loss: 3.1466, Test Accuracy: 44.0801%\n",
      "Epoch 35/100-------------------\n",
      "Training Loss: 0.1988, Training Accuracy: 97.4474%\n",
      "Test Loss: 3.1691, Test Accuracy: 44.0628%\n",
      "Epoch 36/100-------------------\n",
      "Training Loss: 0.1962, Training Accuracy: 97.5142%\n",
      "Test Loss: 3.2078, Test Accuracy: 43.9075%\n",
      "Epoch 37/100-------------------\n",
      "Training Loss: 0.1900, Training Accuracy: 97.6476%\n",
      "Test Loss: 3.2431, Test Accuracy: 43.8212%\n",
      "Epoch 38/100-------------------\n",
      "Training Loss: 0.1800, Training Accuracy: 97.8812%\n",
      "Test Loss: 3.2591, Test Accuracy: 43.9420%\n",
      "Epoch 39/100-------------------\n",
      "Training Loss: 0.1836, Training Accuracy: 97.6310%\n",
      "Test Loss: 3.3080, Test Accuracy: 44.1664%\n",
      "Epoch 40/100-------------------\n",
      "Training Loss: 0.1717, Training Accuracy: 97.7978%\n",
      "Test Loss: 3.3477, Test Accuracy: 43.5796%\n",
      "Epoch 41/100-------------------\n",
      "Training Loss: 0.1629, Training Accuracy: 98.0814%\n",
      "Test Loss: 3.3560, Test Accuracy: 43.8039%\n",
      "Epoch 42/100-------------------\n",
      "Training Loss: 0.1581, Training Accuracy: 98.0314%\n",
      "Test Loss: 3.4005, Test Accuracy: 43.1999%\n",
      "Epoch 43/100-------------------\n",
      "Training Loss: 0.1530, Training Accuracy: 98.2149%\n",
      "Test Loss: 3.4271, Test Accuracy: 43.4588%\n",
      "Epoch 44/100-------------------\n",
      "Training Loss: 0.1521, Training Accuracy: 97.9146%\n",
      "Test Loss: 3.4691, Test Accuracy: 43.4415%\n",
      "Epoch 45/100-------------------\n",
      "Training Loss: 0.1459, Training Accuracy: 98.0480%\n",
      "Test Loss: 3.4952, Test Accuracy: 43.0273%\n",
      "Epoch 46/100-------------------\n",
      "Training Loss: 0.1396, Training Accuracy: 98.2816%\n",
      "Test Loss: 3.5253, Test Accuracy: 43.0790%\n",
      "Epoch 47/100-------------------\n",
      "Training Loss: 0.1381, Training Accuracy: 98.1648%\n",
      "Test Loss: 3.5607, Test Accuracy: 43.0963%\n",
      "Epoch 48/100-------------------\n",
      "Training Loss: 0.1350, Training Accuracy: 98.3317%\n",
      "Test Loss: 3.5923, Test Accuracy: 43.1653%\n",
      "Epoch 49/100-------------------\n",
      "Training Loss: 0.1298, Training Accuracy: 98.4318%\n",
      "Test Loss: 3.6231, Test Accuracy: 43.1826%\n",
      "Epoch 50/100-------------------\n",
      "Training Loss: 0.1260, Training Accuracy: 98.4151%\n",
      "Test Loss: 3.6729, Test Accuracy: 42.9928%\n",
      "Epoch 51/100-------------------\n",
      "Training Loss: 0.1271, Training Accuracy: 98.2649%\n",
      "Test Loss: 3.6973, Test Accuracy: 43.1308%\n",
      "Epoch 52/100-------------------\n",
      "Training Loss: 0.1205, Training Accuracy: 98.3984%\n",
      "Test Loss: 3.7299, Test Accuracy: 42.7339%\n",
      "Epoch 53/100-------------------\n",
      "Training Loss: 0.1172, Training Accuracy: 98.3817%\n",
      "Test Loss: 3.7587, Test Accuracy: 42.8202%\n",
      "Epoch 54/100-------------------\n",
      "Training Loss: 0.1130, Training Accuracy: 98.4985%\n",
      "Test Loss: 3.7812, Test Accuracy: 42.7511%\n",
      "Epoch 55/100-------------------\n",
      "Training Loss: 0.1112, Training Accuracy: 98.4651%\n",
      "Test Loss: 3.8208, Test Accuracy: 42.6648%\n",
      "Epoch 56/100-------------------\n",
      "Training Loss: 0.1084, Training Accuracy: 98.4651%\n",
      "Test Loss: 3.8541, Test Accuracy: 42.6476%\n",
      "Epoch 57/100-------------------\n",
      "Training Loss: 0.1079, Training Accuracy: 98.3984%\n",
      "Test Loss: 3.8832, Test Accuracy: 42.7684%\n",
      "Epoch 58/100-------------------\n",
      "Training Loss: 0.1058, Training Accuracy: 98.4651%\n",
      "Test Loss: 3.9136, Test Accuracy: 43.0445%\n",
      "Epoch 59/100-------------------\n",
      "Training Loss: 0.1031, Training Accuracy: 98.4985%\n",
      "Test Loss: 3.9466, Test Accuracy: 42.4922%\n",
      "Epoch 60/100-------------------\n",
      "Training Loss: 0.1015, Training Accuracy: 98.4484%\n",
      "Test Loss: 3.9809, Test Accuracy: 42.5958%\n",
      "Epoch 61/100-------------------\n",
      "Training Loss: 0.0956, Training Accuracy: 98.5986%\n",
      "Test Loss: 4.0005, Test Accuracy: 42.7339%\n",
      "Epoch 62/100-------------------\n",
      "Training Loss: 0.0935, Training Accuracy: 98.6153%\n",
      "Test Loss: 4.0375, Test Accuracy: 42.4922%\n",
      "Epoch 63/100-------------------\n",
      "Training Loss: 0.0911, Training Accuracy: 98.5986%\n",
      "Test Loss: 4.0701, Test Accuracy: 42.4405%\n",
      "Epoch 64/100-------------------\n",
      "Training Loss: 0.0892, Training Accuracy: 98.5152%\n",
      "Test Loss: 4.0873, Test Accuracy: 42.6303%\n",
      "Epoch 65/100-------------------\n",
      "Training Loss: 0.0890, Training Accuracy: 98.6987%\n",
      "Test Loss: 4.1331, Test Accuracy: 42.5095%\n",
      "Epoch 66/100-------------------\n",
      "Training Loss: 0.0878, Training Accuracy: 98.6653%\n",
      "Test Loss: 4.1534, Test Accuracy: 42.6476%\n",
      "Epoch 67/100-------------------\n",
      "Training Loss: 0.0864, Training Accuracy: 98.6820%\n",
      "Test Loss: 4.2014, Test Accuracy: 42.4232%\n",
      "Epoch 68/100-------------------\n",
      "Training Loss: 0.0827, Training Accuracy: 98.7654%\n",
      "Test Loss: 4.2221, Test Accuracy: 42.1988%\n",
      "Epoch 69/100-------------------\n",
      "Training Loss: 0.0846, Training Accuracy: 98.5152%\n",
      "Test Loss: 4.2609, Test Accuracy: 41.9745%\n",
      "Epoch 70/100-------------------\n",
      "Training Loss: 0.0789, Training Accuracy: 98.8155%\n",
      "Test Loss: 4.2695, Test Accuracy: 41.9745%\n",
      "Epoch 71/100-------------------\n",
      "Training Loss: 0.0794, Training Accuracy: 98.6486%\n",
      "Test Loss: 4.3084, Test Accuracy: 42.1298%\n",
      "Epoch 72/100-------------------\n",
      "Training Loss: 0.0775, Training Accuracy: 98.7654%\n",
      "Test Loss: 4.3216, Test Accuracy: 42.3542%\n",
      "Epoch 73/100-------------------\n",
      "Training Loss: 0.0748, Training Accuracy: 98.7321%\n",
      "Test Loss: 4.3650, Test Accuracy: 42.3369%\n",
      "Epoch 74/100-------------------\n",
      "Training Loss: 0.0726, Training Accuracy: 98.8322%\n",
      "Test Loss: 4.3928, Test Accuracy: 42.3887%\n",
      "Epoch 75/100-------------------\n",
      "Training Loss: 0.0785, Training Accuracy: 98.4985%\n",
      "Test Loss: 4.4381, Test Accuracy: 41.9572%\n",
      "Epoch 76/100-------------------\n",
      "Training Loss: 0.0739, Training Accuracy: 98.6820%\n",
      "Test Loss: 4.4575, Test Accuracy: 41.8709%\n",
      "Epoch 77/100-------------------\n",
      "Training Loss: 0.0708, Training Accuracy: 98.7321%\n",
      "Test Loss: 4.4832, Test Accuracy: 42.0953%\n",
      "Epoch 78/100-------------------\n",
      "Training Loss: 0.0713, Training Accuracy: 98.8322%\n",
      "Test Loss: 4.5174, Test Accuracy: 41.9917%\n",
      "Epoch 79/100-------------------\n",
      "Training Loss: 0.0746, Training Accuracy: 98.5652%\n",
      "Test Loss: 4.5591, Test Accuracy: 41.8882%\n",
      "Epoch 80/100-------------------\n",
      "Training Loss: 0.0666, Training Accuracy: 98.9156%\n",
      "Test Loss: 4.5761, Test Accuracy: 41.7156%\n",
      "Epoch 81/100-------------------\n",
      "Training Loss: 0.0645, Training Accuracy: 98.8322%\n",
      "Test Loss: 4.5968, Test Accuracy: 42.1298%\n",
      "Epoch 82/100-------------------\n",
      "Training Loss: 0.0667, Training Accuracy: 98.7321%\n",
      "Test Loss: 4.6298, Test Accuracy: 42.1988%\n",
      "Epoch 83/100-------------------\n",
      "Training Loss: 0.0643, Training Accuracy: 98.8822%\n",
      "Test Loss: 4.6574, Test Accuracy: 42.1988%\n",
      "Epoch 84/100-------------------\n",
      "Training Loss: 0.0622, Training Accuracy: 98.8322%\n",
      "Test Loss: 4.6842, Test Accuracy: 42.0262%\n",
      "Epoch 85/100-------------------\n",
      "Training Loss: 0.0642, Training Accuracy: 98.7321%\n",
      "Test Loss: 4.7129, Test Accuracy: 42.0780%\n",
      "Epoch 86/100-------------------\n",
      "Training Loss: 0.0616, Training Accuracy: 98.8989%\n",
      "Test Loss: 4.7415, Test Accuracy: 41.8536%\n",
      "Epoch 87/100-------------------\n",
      "Training Loss: 0.0596, Training Accuracy: 98.8989%\n",
      "Test Loss: 4.7607, Test Accuracy: 42.0435%\n",
      "Epoch 88/100-------------------\n",
      "Training Loss: 0.0637, Training Accuracy: 98.8155%\n",
      "Test Loss: 4.8083, Test Accuracy: 41.7846%\n",
      "Epoch 89/100-------------------\n",
      "Training Loss: 0.0573, Training Accuracy: 98.9489%\n",
      "Test Loss: 4.8205, Test Accuracy: 41.9399%\n",
      "Epoch 90/100-------------------\n",
      "Training Loss: 0.0566, Training Accuracy: 98.9489%\n",
      "Test Loss: 4.8551, Test Accuracy: 41.5430%\n",
      "Epoch 91/100-------------------\n",
      "Training Loss: 0.0564, Training Accuracy: 98.9990%\n",
      "Test Loss: 4.8710, Test Accuracy: 41.7501%\n",
      "Epoch 92/100-------------------\n",
      "Training Loss: 0.0568, Training Accuracy: 98.9823%\n",
      "Test Loss: 4.9125, Test Accuracy: 41.7673%\n",
      "Epoch 93/100-------------------\n",
      "Training Loss: 0.0554, Training Accuracy: 98.9823%\n",
      "Test Loss: 4.9364, Test Accuracy: 41.4567%\n",
      "Epoch 94/100-------------------\n",
      "Training Loss: 0.0568, Training Accuracy: 98.8322%\n",
      "Test Loss: 4.9542, Test Accuracy: 41.6465%\n",
      "Epoch 95/100-------------------\n",
      "Training Loss: 0.0539, Training Accuracy: 99.0157%\n",
      "Test Loss: 4.9979, Test Accuracy: 41.3531%\n",
      "Epoch 96/100-------------------\n",
      "Training Loss: 0.0540, Training Accuracy: 98.9990%\n",
      "Test Loss: 5.0316, Test Accuracy: 41.3531%\n",
      "Epoch 97/100-------------------\n",
      "Training Loss: 0.0544, Training Accuracy: 98.8322%\n",
      "Test Loss: 5.0463, Test Accuracy: 41.7328%\n",
      "Epoch 98/100-------------------\n",
      "Training Loss: 0.0507, Training Accuracy: 98.9156%\n",
      "Test Loss: 5.0718, Test Accuracy: 41.4912%\n",
      "Epoch 99/100-------------------\n",
      "Training Loss: 0.0494, Training Accuracy: 99.0657%\n",
      "Test Loss: 5.0967, Test Accuracy: 41.8882%\n",
      "Epoch 100/100-------------------\n",
      "Training Loss: 0.0486, Training Accuracy: 99.0824%\n",
      "Test Loss: 5.1311, Test Accuracy: 41.1805%\n",
      "Best Test Accuracy: 46.3928%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.8163, -1.4979, -0.0926,  ..., -0.9268,  0.3362, -0.3397],\n",
       "                      [-0.6414, -3.0858,  0.2448,  ..., -0.1183, -0.3665, -1.7412],\n",
       "                      [-2.0676, -2.9883, -0.1602,  ..., -0.9300, -2.5451,  0.3916],\n",
       "                      ...,\n",
       "                      [ 0.9792,  1.6769, -4.3831,  ...,  3.5402, -0.9775, -2.5518],\n",
       "                      [-4.0948, -2.0613, -0.3954,  ...,  0.2688, -1.4746, -0.4283],\n",
       "                      [-1.1528, -1.6002,  0.2506,  ..., -1.4795, -1.5225, -0.1176]],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-7.0941e-02, -3.2560e-01, -8.5631e-02, -5.7371e-01, -5.1653e-01,\n",
       "                      -3.6832e-01, -5.5124e-01, -2.8476e-01,  2.5166e-01, -7.4276e-01,\n",
       "                       1.2000e-01, -7.2834e-01, -4.2010e-01, -1.0039e-01, -4.1842e-01,\n",
       "                       7.8252e-03, -5.0166e-01, -1.1946e-01, -1.1594e-01, -1.6041e-01,\n",
       "                      -6.3405e-01, -3.3109e-01, -2.7103e-01, -1.8670e-01, -1.9331e-01,\n",
       "                      -1.9377e-01, -1.9010e-01, -4.6020e-01, -1.2398e-01, -5.3441e-02,\n",
       "                      -2.1267e-01,  3.1552e-02, -2.4764e-01, -4.0556e-01, -2.3747e-01,\n",
       "                      -6.4541e-01, -6.2110e-01, -3.9623e-01,  6.6974e-02,  1.7711e-01,\n",
       "                       3.6554e-02, -1.1300e+00, -2.0126e-01, -3.5647e-01, -2.0115e-01,\n",
       "                      -2.3677e-01, -6.4827e-01, -2.7861e-01, -1.0912e-01, -5.4556e-01,\n",
       "                       1.8906e-02, -6.2241e-01, -7.4081e-01, -5.8631e-01, -8.8122e-01,\n",
       "                      -3.3620e-01, -1.1836e+00, -3.3406e-01, -9.2950e-02,  1.9334e-01,\n",
       "                      -3.7824e-01, -6.2433e-02, -2.3426e-01, -7.2763e-03,  2.8368e-01,\n",
       "                       5.0251e-02, -4.0471e-01, -3.2722e-01,  1.4213e-01, -8.0474e-02,\n",
       "                      -1.0407e-01,  2.5797e-01, -6.6427e-01, -1.4244e-01, -1.7148e-01,\n",
       "                      -3.0078e-01, -4.6608e-01, -1.0478e-01, -3.2599e-01, -1.0628e-01,\n",
       "                      -3.8488e-01,  8.1916e-02, -9.8154e-01, -2.9668e-01, -5.5678e-01,\n",
       "                      -2.1533e-01, -2.4940e-01, -7.4771e-01,  7.9733e-02, -4.5421e-01,\n",
       "                      -1.6623e-01, -3.3045e-01, -3.8132e-01, -1.0895e-01,  3.2402e-01,\n",
       "                      -4.0290e-01,  4.3954e-02, -5.1612e-01,  3.0762e-01,  4.5691e-01,\n",
       "                      -4.7040e-01, -2.5826e-01, -1.1850e-01, -5.7412e-01, -5.9077e-02,\n",
       "                      -3.7957e-01, -1.0608e-01, -3.1310e-01, -5.5777e-01, -2.7126e-01,\n",
       "                      -1.6529e-01, -2.7753e-01,  1.8394e-01, -1.8614e-01, -3.1332e-01,\n",
       "                      -4.0189e-01, -4.6091e-01, -3.7524e-01,  2.3904e-01, -4.1393e-01,\n",
       "                      -2.1147e-01, -5.3739e-01, -5.6745e-01, -6.1564e-01, -3.8002e-01,\n",
       "                      -2.4230e-01, -1.2210e-01,  4.7914e-01, -1.5691e-01, -3.8978e-01,\n",
       "                       7.5856e-02,  1.1879e-02, -5.5965e-01, -2.5073e-01, -1.6333e-01,\n",
       "                      -2.8879e-01, -1.3815e-01, -7.4711e-01, -6.4398e-01, -1.5263e-01,\n",
       "                      -6.5677e-02,  4.1725e-01, -2.7558e-01,  7.5579e-02, -5.7182e-02,\n",
       "                       8.4520e-02, -2.0145e-01, -1.5822e-01, -4.2065e-01, -1.8926e-01,\n",
       "                      -6.4002e-01, -5.4664e-01,  9.3965e-02, -2.0190e-01, -5.8878e-01,\n",
       "                       5.2559e-02, -3.1796e-01, -2.0277e-01, -7.6326e-01, -7.0944e-01,\n",
       "                      -5.3090e-01, -2.3384e-01,  2.1633e-01, -4.0964e-01, -2.3862e-01,\n",
       "                      -6.1278e-01, -1.7921e-01, -4.1709e-01, -4.9913e-01, -4.1288e-01,\n",
       "                       1.8789e-01, -1.3292e-01, -1.1436e-01,  1.1935e-01, -1.4394e-01,\n",
       "                      -2.2859e-01, -4.0728e-01, -8.0148e-01, -4.0794e-01, -4.3402e-01,\n",
       "                      -9.7563e-01, -2.1055e-01, -4.6398e-01,  1.4921e-01, -7.3898e-01,\n",
       "                      -5.6082e-01, -5.7465e-01, -7.7688e-01, -2.6335e-01, -1.0692e+00,\n",
       "                       2.7330e-01, -9.6402e-01, -1.8160e-02, -4.2293e-01, -1.0344e-03,\n",
       "                      -8.9187e-02,  2.5320e-01, -2.9356e-01, -1.0535e-01, -2.2924e-01],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.concept_bottleneck.train import TrainFn, TestFn, run_epochs\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_fn: TrainFn = lambda model: train(\n",
    "    model, training_dataloader, loss_fn, optimizer, device\n",
    ")\n",
    "test_fn: TestFn = lambda model, dataloader: test(model, dataloader, loss_fn, device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "run_epochs(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_fn,\n",
    "    test_fn,\n",
    "    training_dataloader,\n",
    "    test_dataloader,\n",
    "    save_name=\"attributes-to-class.pth\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d4040fe446a930194e7f49e706fe5ca82fc3ae21142ec3efeed3554a6698e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
